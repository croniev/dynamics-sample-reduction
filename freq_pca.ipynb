{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "cce6ef85",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from itertools import product\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nfrom scipy.fftpack import dct\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "661ad528",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "train_data = datasets.MNIST(\n    root='data',\n    train=True,\n    transform=ToTensor(),\n    download=True,\n)\ntest_data = datasets.MNIST(\n    root='data',\n    train=False,\n    transform=ToTensor(),\n)\ndata_len = {'train': len(train_data), 'test': len(test_data)}"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "378a3c3e",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def apply_dynamics(sample, iters, f_act, vel, v_o, v_m):\n    working_sample = np.copy(sample)\n\n    for i in range(iters):\n        v_s = vel(i, iters)\n        # apply doppler effect to sample\n        working_sample = ((v_m + v_o) / (v_m + v_s)) * working_sample  # doppler-effect\n        working_sample = f_act(working_sample)  # activation function\n\n    return working_sample"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "fddddb33",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# vel_s is negative if moving towards observer\ndef v1(x, n):\n    return -(x + 1) / (n / 3)\ndef v2(x, n):\n    return x\ndef v3(x, n):\n    return 1.5\n\ndef relu(x):\n    return np.maximum(0, x)\nrelu = np.vectorize(relu)\n# sigmoid = np.vectorize(lambda x: 1 / (1 + np.exp(-x)))\ntanh = np.vectorize(np.tanh)\n\nv_o = 0  # positive if moving towards source\nv_m = 5.022  # small tail to avoid division errors."
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "6b44de11",
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Symbol Dynamics: 0.0%\n(200, 784)\nStandardising Results\nPerforming PCA\n(200, 15)\nexport DF: (200, 3)\n"
                }
            ],
            "source": "iters = 15\nf_act = relu  # relu, tanh\namount_data = 200\npca_components = 15\n\n# Symbol dynamics\narr_dyn = np.array([])\nfor i in range(amount_data):\n    if i % 500 == 0:\n        print(f\"Symbol Dynamics: {i/amount_data}%\")\n    freq_sample = dct(dct(train_data[i][0][0].numpy().T, norm='ortho').T, norm='ortho')  # decompose sample\n    dyn_sample = apply_dynamics(freq_sample, iters, f_act, v2, v_o, v_m)\n    if i == 0:\n        arr_dyn = np.array([dyn_sample.reshape((-1))])\n    else:\n        arr_dyn = np.concat((arr_dyn, [dyn_sample.reshape((-1))]))\n\nprint(arr_dyn.shape)\ndf_dyn = pd.DataFrame(arr_dyn, columns=[f'x{g}' for g in range(dyn_sample.size)])\n\n# PCA\nprint(\"Standardising Results\")\nscaling = StandardScaler()\nscaling.fit(df_dyn)\nscaled_data = scaling.transform(df_dyn)\n\nprint(\"Performing PCA\")\nmodel = PCA(n_components=pca_components)\nmodel.fit(scaled_data)\nresults = model.transform(scaled_data)\nprint(results.shape)\n\n# Save data\ndf_results = pd.DataFrame(columns=['data', 'label', 'train'])\nfor i, r in enumerate(results):\n    result_str = np.array2string(r, separator=',')\n    df_results = pd.concat([df_results, pd.DataFrame([[result_str, train_data[i][1], i < amount_data * 0.85]], columns=df_results.columns)], ignore_index=True)\n\ndf_results.to_csv(f'freq_RedData/freq_pca{pca_components}_RedData_{iters}i_{f_act.__name__}.csv', index=False)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {},
            "toc_section_display": true,
            "toc_window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}